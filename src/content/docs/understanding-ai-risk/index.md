---
title: Understanding AI Risk
description: The case for AI existential risk and key uncertainties
sidebar:
  order: 0
---

This section presents the core arguments for why advanced AI might pose existential or catastrophic risks, the key models for thinking about AI development trajectories, and different worldviews on the topic.

## The Core Argument

The basic case for AI existential risk involves several steps, each with associated uncertainties:

1. **[Timelines](/understanding-ai-risk/core-argument/timelines)** - Advanced AI may arrive sooner than expected
2. **[Capabilities](/understanding-ai-risk/core-argument/capabilities)** - AI systems may become very powerful
3. **[Takeoff Speed](/understanding-ai-risk/core-argument/takeoff)** - Progress may be fast enough to prevent adequate response
4. **[Goal-Directedness](/understanding-ai-risk/core-argument/goal-directedness)** - AI may pursue goals persistently
5. **[Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty)** - Ensuring AI pursues the right goals is hard
6. **[Catastrophic Outcomes](/understanding-ai-risk/core-argument/catastrophe)** - Misaligned AI could cause irreversible harm
7. **[Coordination Challenges](/understanding-ai-risk/core-argument/coordination)** - Collective action problems may prevent safe development

## Risk Models

Different frameworks for thinking about how AI risk develops:

- [Capability-Alignment Race](/understanding-ai-risk/models/capability-alignment-race) - Will alignment keep pace with capabilities?
- [Multi-Actor Landscape](/understanding-ai-risk/models/multi-actor-landscape) - How do different actors interact?
- [Technical Pathways](/understanding-ai-risk/models/technical-pathways) - What technical routes might lead to risk?
- [Societal Response](/understanding-ai-risk/models/societal-response) - How might society react to AI developments?
- [Feedback Loops](/understanding-ai-risk/models/feedback-loops) - What dynamics accelerate or mitigate risk?
- [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties) - What don't we know that matters most?

## Worldviews

Different perspectives on AI risk vary in their probability estimates and priority recommendations:

- [High-Risk View](/understanding-ai-risk/worldviews/doomer) - Those who assign high probability to catastrophe
- [Optimistic View](/understanding-ai-risk/worldviews/optimistic) - Those who see AI development as more likely positive
- [Governance-Focused View](/understanding-ai-risk/worldviews/governance-focused) - Those who prioritize institutional responses
- [Long Timelines View](/understanding-ai-risk/worldviews/long-timelines) - Those who expect more time to prepare

## How to Use This Section

If you're new to AI risk:
1. Start with the [Core Argument](/understanding-ai-risk/core-argument/) to understand the basic case
2. Review [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties) to understand key debates
3. Explore different [Worldviews](/understanding-ai-risk/worldviews/) to see how people weigh the evidence differently

If you're trying to form your own view:
- Consider which steps in the core argument you find most and least convincing
- Identify which uncertainties most affect your probability estimates
- Use the worldviews to understand the range of reasonable positions

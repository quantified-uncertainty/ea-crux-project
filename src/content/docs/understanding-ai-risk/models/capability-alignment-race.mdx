---
title: Capability-Alignment Race Model
description: Risk emerges from the gap between capability progress and safety/governance readiness.
tableOfContents: false
---

import CauseEffectGraph from '../../../../components/CauseEffectGraph';

<style>{`
  .breakout {
    margin-left: -300px;
    margin-right: -300px;
    width: calc(100% + 600px);
  }
  @media (max-width: 1400px) {
    .breakout {
      margin-left: -200px;
      margin-right: -200px;
      width: calc(100% + 400px);
    }
  }
  @media (max-width: 1100px) {
    .breakout {
      margin-left: -100px;
      margin-right: -100px;
      width: calc(100% + 200px);
    }
  }
  @media (max-width: 800px) {
    .breakout {
      margin-left: 0;
      margin-right: 0;
      width: 100%;
    }
  }
`}</style>

**Core thesis**: Risk emerges from the gap between capability progress and safety/governance readiness. The race between capabilities and alignment determines outcomes.

<div class="breakout">
<CauseEffectGraph
  client:load
  height={900}
  fitViewPadding={0.05}
  initialNodes={[
    {
      id: 'compute',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Compute Available',
        description: 'FLOP/s available to leading labs.',
        type: 'cause',
        confidence: 26,
        confidenceLabel: 'log₁₀ FLOP/s',
        details: 'Training compute for frontier models. Currently ~10²⁶ FLOP for largest runs. Doubling every 6-12 months.',
        relatedConcepts: ['Scaling laws', 'GPU clusters', 'Training runs']
      }
    },
    {
      id: 'algorithmic',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Algorithmic Efficiency',
        description: 'Improvement over 2024 baseline.',
        type: 'cause',
        confidence: 2,
        confidenceLabel: 'x baseline',
        details: 'Algorithmic improvements compound with compute. Architecture innovations, training techniques, data efficiency.',
        relatedConcepts: ['Transformers', 'MoE', 'Chinchilla scaling']
      }
    },
    {
      id: 'frontier-labs',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Frontier Lab Lead',
        description: 'Lead time from 1st to 2nd place lab.',
        type: 'cause',
        confidence: 6,
        confidenceLabel: 'months',
        details: 'How concentrated is the frontier? Smaller lead = more racing pressure. Currently ~6 months between top labs.',
        relatedConcepts: ['Racing dynamics', 'Concentration', 'Competition']
      }
    },
    {
      id: 'opensource-lag',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Open-Source Lag',
        description: 'Time from frontier to open-source.',
        type: 'cause',
        confidence: 18,
        confidenceLabel: 'months',
        details: 'How quickly do capabilities proliferate? Affects misuse risk and governance difficulty. Currently ~18 months.',
        relatedConcepts: ['Llama', 'Mistral', 'Proliferation']
      }
    },
    {
      id: 'capability-level',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Frontier Capability',
        description: 'Current frontier model capabilities.',
        type: 'intermediate',
        confidence: 0.7,
        confidenceLabel: 'vs. human expert',
        details: 'Aggregate capability level of best models. Currently ~70% of human expert on most cognitive tasks.',
        relatedConcepts: ['Benchmarks', 'MMLU', 'Coding', 'Reasoning']
      }
    },
    {
      id: 'interp',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Interpretability',
        description: 'Understanding of model internals.',
        type: 'cause',
        confidence: 0.15,
        confidenceLabel: 'coverage',
        details: 'What fraction of model behavior can we mechanistically explain? Currently ~15% for key circuits.',
        relatedConcepts: ['Sparse autoencoders', 'Circuits', 'Features']
      }
    },
    {
      id: 'oversight',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Scalable Oversight',
        description: 'Techniques to supervise superhuman AI.',
        type: 'cause',
        confidence: 0.3,
        confidenceLabel: 'maturity',
        details: 'Debate, recursive reward modeling, etc. Currently ~30% mature. Critical for superhuman alignment.',
        relatedConcepts: ['Debate', 'Amplification', 'Weak-to-strong']
      }
    },
    {
      id: 'alignment-tax',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Alignment Tax',
        description: 'Capability cost of safety measures.',
        type: 'cause',
        confidence: 0.15,
        confidenceLabel: 'capability loss',
        details: 'How much capability do you sacrifice for safety? Currently ~15%. Lower tax = more adoption.',
        relatedConcepts: ['RLHF overhead', 'Safety fine-tuning', 'Refusals']
      }
    },
    {
      id: 'deception-detect',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Deception Detection',
        description: 'Ability to detect deceptive alignment.',
        type: 'cause',
        confidence: 0.2,
        confidenceLabel: 'capability',
        details: 'Can we tell if a model is strategically deceiving us? Currently ~20% reliable.',
        relatedConcepts: ['Sleeper agents', 'Trojans', 'Honeypots']
      }
    },
    {
      id: 'alignment-gap',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Capability-Alignment Gap',
        description: 'How far ahead are capabilities vs. alignment?',
        type: 'intermediate',
        confidence: 3,
        confidenceLabel: 'years gap',
        details: 'The core race metric. Currently capabilities ~3 years ahead of alignment. Gap increasing.',
        relatedConcepts: ['Racing', 'Differential progress', 'Safety lag']
      }
    },
    {
      id: 'econ-value',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Economic Value',
        description: 'Annual value of AI capabilities.',
        type: 'cause',
        confidence: 500,
        confidenceLabel: '$B/year',
        details: 'Revenue and productivity gains from AI. Creates deployment pressure. Currently ~$500B/year and growing rapidly.',
        relatedConcepts: ['GDP impact', 'Automation', 'Productivity']
      }
    },
    {
      id: 'arms-race',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Military AI Race',
        description: 'Intensity of AI arms race.',
        type: 'cause',
        confidence: 0.6,
        confidenceLabel: 'intensity (0-1)',
        details: 'US-China military AI competition. Higher intensity = less safety focus. Currently ~0.6.',
        relatedConcepts: ['Autonomous weapons', 'Defense AI', 'Strategic competition']
      }
    },
    {
      id: 'deploy-pressure',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Deployment Pressure',
        description: 'Pressure to deploy quickly.',
        type: 'intermediate',
        confidence: 0.7,
        confidenceLabel: 'intensity (0-1)',
        details: 'Combined economic, military, and competitive pressure. Currently high (~0.7).',
        relatedConcepts: ['Time to market', 'First mover', 'Racing']
      }
    },
    {
      id: 'us-reg',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'US AI Regulation',
        description: 'Stringency of US AI rules.',
        type: 'cause',
        confidence: 0.25,
        confidenceLabel: 'stringency (0-1)',
        details: 'Executive orders, potential legislation. Currently ~0.25 (low). Increasing.',
        relatedConcepts: ['EO 14110', 'Congress', 'NIST']
      }
    },
    {
      id: 'intl-coord',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'International Coordination',
        description: 'Strength of global AI governance.',
        type: 'cause',
        confidence: 0.2,
        confidenceLabel: 'effectiveness (0-1)',
        details: 'Treaties, safety institutes, coordination. Currently ~0.2 (weak).',
        relatedConcepts: ['AI Safety Summit', 'GPAI', 'Treaties']
      }
    },
    {
      id: 'compute-gov',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Compute Governance',
        description: 'Monitoring and control of AI compute.',
        type: 'cause',
        confidence: 0.15,
        confidenceLabel: 'coverage (0-1)',
        details: 'Export controls, KYC for cloud, hardware tracking. Currently ~0.15.',
        relatedConcepts: ['Chip controls', 'Cloud KYC', 'Hardware tracking']
      }
    },
    {
      id: 'public-concern',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Public Concern',
        description: 'Public awareness and worry about AI risk.',
        type: 'cause',
        confidence: 0.4,
        confidenceLabel: 'level (0-1)',
        details: 'Drives political will for regulation. Currently ~0.4 and rising.',
        relatedConcepts: ['Media coverage', 'Polling', 'Advocacy']
      }
    },
    {
      id: 'governance-strength',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Governance Strength',
        description: 'Overall AI governance effectiveness.',
        type: 'intermediate',
        confidence: 0.25,
        confidenceLabel: 'effectiveness (0-1)',
        details: 'Combined domestic and international governance. Currently weak (~0.25).',
        relatedConcepts: ['Regulation', 'Enforcement', 'Standards']
      }
    },
    {
      id: 'warning-shot',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Warning Shot',
        description: 'Probability of visible AI incident.',
        type: 'intermediate',
        confidence: 0.6,
        confidenceLabel: 'P(before TAI)',
        details: 'A significant but recoverable AI accident that galvanizes action. 60% chance before TAI.',
        relatedConcepts: ['Near miss', 'Wake-up call', 'Incident']
      }
    },
    {
      id: 'accident-risk',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Accident Risk',
        description: 'Risk from unintentional misalignment.',
        type: 'intermediate',
        confidence: 0.12,
        confidenceLabel: 'expected loss',
        details: 'Driven by capability-alignment gap and deployment pressure.',
        relatedConcepts: ['Misalignment', 'Mesa-optimization', 'Goal misgeneralization']
      }
    },
    {
      id: 'misuse-risk',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Misuse Risk',
        description: 'Risk from intentional harmful use.',
        type: 'intermediate',
        confidence: 0.08,
        confidenceLabel: 'expected loss',
        details: 'Driven by proliferation and weak governance.',
        relatedConcepts: ['Bioweapons', 'Cyber', 'Manipulation']
      }
    },
    {
      id: 'structural-risk',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Structural Risk',
        description: 'Risk from systemic failures.',
        type: 'intermediate',
        confidence: 0.06,
        confidenceLabel: 'expected loss',
        details: 'Multi-agent dynamics, race to bottom, coordination failures.',
        relatedConcepts: ['Racing', 'Lock-in', 'Collective action']
      }
    },
    {
      id: 'total-risk',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Total X-Risk',
        description: 'Combined existential risk from AI.',
        type: 'effect',
        confidence: 0.25,
        confidenceLabel: 'expected loss',
        details: 'Sum of accident, misuse, and structural risk pathways.',
        relatedConcepts: ['P(doom)', 'Existential risk', 'Catastrophe']
      }
    }
  ]}
  initialEdges={[
    { id: 'e-compute-cap', source: 'compute', target: 'capability-level', data: { impact: 0.35 } },
    { id: 'e-algo-cap', source: 'algorithmic', target: 'capability-level', data: { impact: 0.35 } },
    { id: 'e-frontier-cap', source: 'frontier-labs', target: 'capability-level', data: { impact: 0.15 } },
    { id: 'e-opensource-cap', source: 'opensource-lag', target: 'capability-level', data: { impact: 0.15 } },
    { id: 'e-interp-gap', source: 'interp', target: 'alignment-gap', data: { impact: 0.25 } },
    { id: 'e-oversight-gap', source: 'oversight', target: 'alignment-gap', data: { impact: 0.25 } },
    { id: 'e-tax-gap', source: 'alignment-tax', target: 'alignment-gap', data: { impact: 0.15 } },
    { id: 'e-deception-gap', source: 'deception-detect', target: 'alignment-gap', data: { impact: 0.20 } },
    { id: 'e-cap-gap', source: 'capability-level', target: 'alignment-gap', data: { impact: 0.15 } },
    { id: 'e-econ-deploy', source: 'econ-value', target: 'deploy-pressure', data: { impact: 0.40 } },
    { id: 'e-arms-deploy', source: 'arms-race', target: 'deploy-pressure', data: { impact: 0.35 } },
    { id: 'e-frontier-deploy', source: 'frontier-labs', target: 'deploy-pressure', data: { impact: 0.25 } },
    { id: 'e-us-gov', source: 'us-reg', target: 'governance-strength', data: { impact: 0.30 } },
    { id: 'e-intl-gov', source: 'intl-coord', target: 'governance-strength', data: { impact: 0.25 } },
    { id: 'e-compute-gov', source: 'compute-gov', target: 'governance-strength', data: { impact: 0.25 } },
    { id: 'e-public-gov', source: 'public-concern', target: 'governance-strength', data: { impact: 0.20 } },
    { id: 'e-cap-warning', source: 'capability-level', target: 'warning-shot', data: { impact: 0.50 } },
    { id: 'e-deploy-warning', source: 'deploy-pressure', target: 'warning-shot', data: { impact: 0.50 } },
    { id: 'e-warning-public', source: 'warning-shot', target: 'public-concern', data: { impact: 0.60 }, style: { strokeDasharray: '5,5' } },
    { id: 'e-gap-accident', source: 'alignment-gap', target: 'accident-risk', data: { impact: 0.50 } },
    { id: 'e-deploy-accident', source: 'deploy-pressure', target: 'accident-risk', data: { impact: 0.30 } },
    { id: 'e-gov-accident', source: 'governance-strength', target: 'accident-risk', data: { impact: 0.20 } },
    { id: 'e-opensource-misuse', source: 'opensource-lag', target: 'misuse-risk', data: { impact: 0.40 } },
    { id: 'e-cap-misuse', source: 'capability-level', target: 'misuse-risk', data: { impact: 0.30 } },
    { id: 'e-gov-misuse', source: 'governance-strength', target: 'misuse-risk', data: { impact: 0.30 } },
    { id: 'e-deploy-struct', source: 'deploy-pressure', target: 'structural-risk', data: { impact: 0.35 } },
    { id: 'e-arms-struct', source: 'arms-race', target: 'structural-risk', data: { impact: 0.35 } },
    { id: 'e-gov-struct', source: 'governance-strength', target: 'structural-risk', data: { impact: 0.30 } },
    { id: 'e-accident-total', source: 'accident-risk', target: 'total-risk', data: { impact: 0.45 } },
    { id: 'e-misuse-total', source: 'misuse-risk', target: 'total-risk', data: { impact: 0.30 } },
    { id: 'e-struct-total', source: 'structural-risk', target: 'total-risk', data: { impact: 0.25 } }
  ]}
/>
</div>

## Key Dynamics

1. **Capability → Economic pressure → Deployment speed → Accident risk**
2. **Warning shots → Public concern → Regulation → Deployment delay** (protective feedback loop, shown dashed)
3. **Arms race → Reduced safety investment → Alignment lag → Risk**
4. **Interpretability → Better alignment → Lower alignment tax → More safety adoption**

## Categories

| Category | Nodes | Key Variables |
|----------|-------|---------------|
| **Capability Development** | 5 | Compute, algorithms, lab lead, open-source lag |
| **Alignment Progress** | 4 | Interpretability, oversight, alignment tax, deception detection |
| **Deployment & Incentives** | 3 | Economic value, military race, deployment pressure |
| **Governance** | 5 | US regulation, international coordination, compute governance, public concern |
| **Risk Outcomes** | 4 | Warning shot, accident, misuse, structural risk |

## Full Variable List

This diagram simplifies the full model. The complete Capability-Alignment Race Model includes:

**Capability Development (12 variables)**: Compute available, algorithmic efficiency, training data, synthetic data, hardware progress, chip export controls, cloud compute governance, number of frontier labs, 1st-to-2nd place lead time, frontier-to-open-source lag, capability jump size, pre-vs-post-training balance.

**Alignment Progress (10 variables)**: Interpretability coverage, scalable oversight maturity, alignment tax, adversarial robustness, OOD generalization, deception detection, mesa-optimizer control, agent foundations progress, alignment funding, safety researcher talent pool.

**Deployment & Incentives (8 variables)**: Economic value, military arms race intensity, commercial pressure, regulatory compliance costs, liability regime, deployment delay, API vs open-weight ratio, corporate safety culture.

**Governance (11 variables)**: US/China/EU regulation stringency, international treaties, compute monitoring, whistleblower protections, third-party auditing, government safety funding, regulatory capture, major power coordination, public awareness.

**Risk Outcomes (9 variables)**: Misalignment severity, power-seeking probability, deception vs detection, rogue deployment, multi-agent conflict, economic disruption, warning shot occurrence, value lock-in, existential risk.

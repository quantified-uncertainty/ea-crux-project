---
title: Is Alignment Fundamentally Hard?
description: Can we reliably get AI systems to do what we want?
sidebar:
  order: 5
---

**The claim**: Aligning advanced AI with human values is a fundamentally difficult problem that we may not solve in time, or at all.

## What Makes Alignment Hard?

| Challenge | Description |
|-----------|-------------|
| **Specification** | We can't fully specify what we want |
| **Scalable oversight** | We can't evaluate superhuman outputs |
| **Robustness** | Alignment must hold in all situations |
| **Deception** | AI might fake alignment |
| **Goodhart** | Optimizing proxies diverges from goals |

## Arguments for "Alignment Is Hard"

- **Specification problem**: Human values are complex, contextual, contradictory
- **Orthogonality**: Intelligence doesn't imply good values
- **Evaluation gap**: Can't verify alignment of smarter-than-human systems
- **Adversarial dynamics**: AI might game any metric we define
- **One-shot problem**: First superintelligence might be only chance

**Key proponents**: MIRI, Yudkowsky, many safety researchers

## Arguments for "Alignment Is Tractable"

- **Current progress**: RLHF, Constitutional AI work reasonably well
- **Natural alignment**: AI trained on human data absorbs human values
- **Iteration**: We can learn from failures and improve
- **Not adversarial**: AI isn't trying to deceive us (yet)
- **AI assistance**: We can use AI to help align AI

**Key proponents**: Many industry researchers, some academics

## Current Techniques

| Technique | Promise | Limitation |
|-----------|---------|------------|
| RLHF | Works today | May not scale; sycophancy |
| Constitutional AI | More scalable | Still limited by constitution |
| Interpretability | Understand AI | May not scale to frontier |
| Debate/oversight | Scale human judgment | Might be gamed |
| Formal verification | Mathematical guarantees | Not applicable to neural nets |

## The Scaling Question

Key crux: **Will current techniques scale to superhuman AI?**

| Scales | Doesn't Scale |
|--------|--------------|
| Alignment is engineering | Alignment requires breakthrough |
| Can iterate our way to solution | Fundamental obstacles |
| AI can help | AI assistance is dangerous |

## Your Crux

Probability that:
- We can align human-level AI with current techniques: ____%
- We can align 10x superhuman AI: ____%
- Alignment will be solved before dangerous AI exists: ____%
- If we fail at alignment, we'll notice before catastrophe: ____%

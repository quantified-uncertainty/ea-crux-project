---
title: The Core Argument
description: Step-by-step case for AI existential risk
sidebar:
  order: 0
---

The case for AI existential risk can be broken into a chain of claims. Each link has associated probabilities and uncertainties. Your overall view on AI risk depends on how you assess each step.

## The Chain of Reasoning

1. **Advanced AI is coming** - Timelines to transformative AI may be short
2. **It will be very powerful** - Capabilities will exceed human abilities in many domains
3. **Development may be fast** - Takeoff speed affects response time
4. **AI will be goal-directed** - Systems will pursue objectives persistently
5. **Alignment is difficult** - Making AI pursue the right goals is technically hard
6. **Failure could be catastrophic** - Misaligned powerful AI could cause irreversible harm
7. **Coordination is hard** - Collective action problems may prevent safe development

## Exploring Each Step

Each page in this section examines one step:

- What's the argument?
- What's the evidence?
- What are the key uncertainties?
- What do different perspectives say?

## How Probabilities Compound

If each step has a probability, the overall risk is (roughly) their product. For example:

| Step | Optimistic | Pessimistic |
|------|------------|-------------|
| Timelines < 2040 | 30% | 70% |
| Capabilities sufficient | 50% | 90% |
| Fast takeoff | 20% | 60% |
| Goal-directed | 40% | 80% |
| Alignment fails | 10% | 50% |
| Catastrophic outcome | 30% | 80% |
| Coordination fails | 50% | 80% |

These numbers are illustrative. Your own estimates matter more than any particular set of numbers.

## Key Disagreements

People disagree most about:
- **Timelines**: When will transformative AI arrive?
- **Alignment difficulty**: How hard is the technical problem?
- **Takeoff speed**: Will there be time to react?
- **Goal structure**: Will AI systems be goal-directed in dangerous ways?

Understanding where you stand on these cruxes helps clarify your overall view.

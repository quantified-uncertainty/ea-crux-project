---
title: Will We Fail to Coordinate?
description: Will competitive dynamics prevent us from developing AI safely?
sidebar:
  order: 7
---

**The claim**: Even if alignment is technically possible, competitive pressures between labs and nations will lead to racing, corner-cutting on safety, and ultimately catastrophe.

## The Coordination Failures

| Failure Mode | Description |
|--------------|-------------|
| **Lab racing** | Companies cut safety corners to ship first |
| **Nation racing** | Countries deprioritize safety for strategic advantage |
| **Proliferation** | Dangerous capabilities spread to bad actors |
| **Regulatory failure** | Governance can't keep up with technology |
| **Value disagreement** | No consensus on what "safe" means |

## Arguments for Coordination Failure

- **First-mover advantage**: Huge incentives to be first
- **Tragedy of commons**: Safety is a public good, under-provided
- **US-China competition**: Geopolitical stakes make cooperation hard
- **Regulatory lag**: Government can't keep up with AI progress
- **Open source**: Can't control spread of capabilities
- **Precedent**: Humans have failed at similar coordination problems

## Arguments for Successful Coordination

- **Mutual destruction**: Everyone loses from catastrophe; motivates cooperation
- **Small number of actors**: Only a few frontier labs to coordinate
- **Visible progress**: International safety institutes, commitments forming
- **Lab incentives**: Reputation, liability, genuine concern for safety
- **Historical precedent**: Nuclear cooperation, ozone layer, others

## Current Coordination Efforts

| Effort | Status | Effectiveness |
|--------|--------|---------------|
| AI Safety Institutes | Growing | Too early to tell |
| Voluntary commitments | Many signed | Unclear enforcement |
| Export controls | Implemented | May backfire |
| International dialogue | Beginning | Limited trust |

## Key Sub-Cruxes

| Question | Pessimist | Optimist |
|----------|-----------|----------|
| US-China cooperation? | Impossible | Achievable |
| Lab self-regulation? | Insufficient | Can work |
| Regulatory effectiveness? | Too slow | Can adapt |
| Open source? | Dangerous | Net positive |
| Global pause feasible? | No | Maybe |

## The Counterfactual

Even if *you* act safely, will others?

| Unilateral action matters | Unilateral action is futile |
|---------------------------|----------------------------|
| Set norms and standards | Others won't follow |
| Buy time for coordination | Just delays the inevitable |
| Demonstrate feasibility | Cedes advantage to less careful |

## Your Crux

Probability that:
- Major AI labs can effectively coordinate on safety: ____%
- US-China can cooperate on AI safety: ____%
- Governance will keep pace with AI progress: ____%
- Racing dynamics can be mitigated: ____%
- Humanity will successfully navigate the AI transition: ____%

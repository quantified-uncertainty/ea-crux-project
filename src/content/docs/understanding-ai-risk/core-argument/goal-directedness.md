---
title: Will AI Be Goal-Directed in Dangerous Ways?
description: Will AI systems pursue goals in ways that conflict with human interests?
sidebar:
  order: 4
---

**The claim**: Advanced AI systems will be goal-directed agents that pursue objectives, and those objectives will tend toward acquiring power and resisting shutdown.

## The Two Sub-Claims

### 4a: AI will be goal-directed (agentic)

| Tool AI View | Agent AI View |
|--------------|---------------|
| AI responds to queries | AI pursues objectives |
| No persistent goals | Has goals across contexts |
| Humans remain in control | AI takes autonomous action |
| Like a calculator | Like an employee (or competitor) |

### 4b: Goals will be problematic (instrumental convergence)

| Weak Convergence | Strong Convergence |
|------------------|-------------------|
| Power-seeking is one option | Power-seeking is default |
| Depends on training | Emerges regardless |
| Can train away | Fundamentally hard to prevent |

## Arguments for Dangerous Goal-Directedness

- **Agency is useful**: Goal-directed systems accomplish more
- **Selection pressure**: Training selects for effective agents
- **Instrumental convergence**: Most goals benefit from power/survival
- **Observed behavior**: Current models show goal-directed tendencies
- **Evolution analogy**: Produced goal-directed agents (humans)

**Theoretical basis**: Omohundro's "Basic AI Drives," Bostrom's instrumental convergence thesis

## Arguments Against

- **Tool AI is sufficient**: We can build useful AI without agency
- **Training shapes behavior**: Can specifically train against power-seeking
- **Weak convergence**: Power-seeking isn't as universal as claimed
- **Current systems aren't agents**: LLMs don't have persistent goals
- **No mesa-optimization**: Trained models won't develop internal optimizers

## The Mesa-Optimization Question

Will trained neural networks develop their own internal goals?

| Mesa-optimization likely | Mesa-optimization unlikely |
|-------------------------|---------------------------|
| Optimization is powerful, will be selected for | Simpler strategies suffice |
| Observed in evolution | Training different from evolution |
| Gets worse with capability | Can prevent through architecture |

## Your Crux

Probability that advanced AI will:
- Have persistent goals across contexts: ____%
- Seek to preserve itself: ____%
- Acquire resources beyond task needs: ____%
- Resist modification by developers: ____%
- These tendencies get stronger with capability: ____%

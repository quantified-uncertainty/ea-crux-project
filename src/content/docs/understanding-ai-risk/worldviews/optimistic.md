---
title: Optimistic Alignment Worldview
description: Alignment is tractable, progress is real, we'll figure it out.
---

**Core belief**: Alignment is a hard but tractable engineering problem. Current progress is real, and with continued effort, we can develop AI safely.

## Characteristic Beliefs

| Crux | Typical Optimist Position |
|------|--------------------------|
| Timelines | Variable (not the key crux) |
| Paradigm | Either way, alignment scales |
| Takeoff | Slow enough to iterate |
| Alignment difficulty | Engineering problem, not fundamental |
| Instrumental convergence | Weak or avoidable through training |
| Deceptive alignment | Unlikely in practice |
| Current techniques | Show real progress, will improve |
| Coordination | Achievable with effort |
| P(doom) | <5% |

## Priority Approaches

Given these beliefs, optimists prioritize:

1. **RLHF/Constitutional AI**: Continue improving what works
2. **Evals & red-teaming**: Catch problems empirically
3. **Lab safety culture**: Get practices right inside labs
4. **AI-assisted alignment**: Use AI to help
5. **Scalable oversight**: Extend human judgment

## Deprioritized Approaches

| Approach | Why less important |
|----------|-------------------|
| Pause advocacy | Unnecessary, may backfire |
| Agent foundations | Too theoretical, slow |
| Compute governance | Overreach, centralization risk |

## Key Proponents

- Many industry safety researchers
- Some academics
- Effective accelerationists (more extreme)

## Strongest Arguments

1. We've made real progress on current models
2. Alarm about future AI relies on speculation
3. Each generation of AI gives us more data to learn from
4. Humans have solved hard problems before
5. Slowing down has real costs (beneficial AI delayed)

## Weakest Points / Common Critiques

- Underrates qualitative shifts at superhuman level
- May be motivated by industry incentives
- Success on current models doesn't guarantee future success
- "We'll figure it out" isn't a plan
- Optimism isn't a strategy

## What Would Change This View?

- Clear failures that current techniques can't address
- Evidence of deceptive behavior in models
- Theoretical arguments showing fundamental barriers
- Demonstrated inability to detect misalignment

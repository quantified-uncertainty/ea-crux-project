---
title: Long-Timelines Technical Worldview
description: We have time for careful research and deep solutions.
---

**Core belief**: Transformative AI is further away than many think. This gives us time for careful, foundational research rather than rushed solutions.

## Characteristic Beliefs

| Crux | Typical Long-Timelines Position |
|------|--------------------------------|
| Timelines | AGI 20-40+ years away |
| Paradigm | May need new paradigms (not just scaling) |
| Takeoff | Slow and observable |
| Alignment difficulty | Hard, but we have time |
| Current research relevance | Uncertain if current LLMs inform future AI |
| Deceptive alignment | Relevant but not imminent |
| P(doom) | 5-20% |

## Priority Approaches

Given these beliefs, long-timelines people prioritize:

1. **Agent foundations**: Time for deep theory
2. **Interpretability**: Build understanding carefully
3. **Foundational research**: First principles approaches
4. **Academic AI safety**: Building the field

## Deprioritized Approaches

| Approach | Why less important |
|----------|-------------------|
| Pause advocacy | Less urgent |
| RLHF improvements | May not transfer to future AI |
| Lab safety culture | Less immediate relevance |

## Distinctive Features

Unlike doomers:
- Less urgency, more patience
- Value theoretical depth over speed
- Skeptical current LLM work transfers

Unlike optimists:
- Still take existential risk seriously
- Don't trust current techniques to scale
- Want robust solutions, not quick fixes

## Strongest Arguments

1. AI predictions have consistently been overoptimistic
2. Current models may not be on path to AGI
3. Rushing leads to shoddy solutions
4. Best research needs time and depth
5. Field-building now pays off later

## Weakest Points / Common Critiques

- Timelines may be shorter than expected
- May miss the window to influence current development
- Current research could become irrelevant if paradigm shifts
- "Patient" research may never be applied
- May be motivated by preferring academic work

## What Would Change This View?

- Dramatic capability jumps in near term
- Clear evidence scaling reaches AGI
- Current LLM work proving highly relevant
- Opportunities to influence current systems

---
title: AI Doomer Worldview
description: Short timelines, hard alignment, high risk.
---

**Core belief**: Advanced AI will be developed soon, alignment is fundamentally hard, and catastrophe is likely unless drastic action is taken.

## Characteristic Beliefs

| Crux | Typical Doomer Position |
|------|------------------------|
| Timelines | AGI likely within 10-15 years |
| Paradigm | Scaling may be sufficient |
| Takeoff | Could be fast (weeks-months) |
| Alignment difficulty | Fundamentally hard, not just engineering |
| Instrumental convergence | Strong and default |
| Deceptive alignment | Significant risk |
| Current techniques | Won't scale to superhuman |
| Coordination | Likely to fail |
| P(doom) | 30-90% |

## Priority Approaches

Given these beliefs, doomers prioritize:

1. **Pause/slowdown advocacy**: Buy time for safety research
2. **Agent foundations**: Need deep theoretical solutions
3. **Interpretability**: Must detect deception
4. **Governance**: Coordinate to prevent racing
5. **Compute governance**: Physical control points

## Deprioritized Approaches

| Approach | Why less important |
|----------|-------------------|
| RLHF improvements | Won't scale to superhuman |
| Lab safety culture | Insufficient without structural change |
| Evals | Can't catch deceptive alignment |
| AI-assisted alignment | Bootstrapping is dangerous |

## Key Proponents

- Eliezer Yudkowsky
- Many MIRI researchers
- Some Conjecture researchers
- Various independent researchers

## Strongest Arguments

1. The default outcome of creating something smarter than us is that it pursues its own goals
2. Alignment must be solved before AGI, not afterâ€”you can't iterate on existential risk
3. Current success with RLHF says nothing about superhuman alignment
4. Racing dynamics are already visible and worsening

## Weakest Points / Common Critiques

- Overconfident in short timelines
- Underrates potential for alignment progress
- Dismisses current safety work too quickly
- Policy proposals may be politically unrealistic
- May be overly influenced by specific intuitions

## What Would Change This View?

- Clear evidence of significant alignment progress
- Long periods without capability jumps
- Coordination success
- Demonstrations that current techniques scale

---
title: Enfeeblement
description: Humanity losing capability and agency through AI dependence
sidebar:
  order: 3
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../components/wiki';

<InfoBox
  type="risk"
  title="Enfeeblement"
  severity="medium-high"
  likelihood="Medium"
  timeframe="Medium-term"
  customFields={[
    { label: "Type", value: "Structural" },
    { label: "Also Called", value: "Human atrophy, skill loss" },
  ]}
/>

## Overview

Enfeeblement refers to humanity gradually losing capabilities, skills, and meaningful agency as AI systems take over more functions. Unlike sudden catastrophe, this is a slow erosion where humans become increasingly dependent on AI, losing the ability to function without it and potentially losing the ability to oversee or redirect AI systems.

The concern is that even aligned, helpful AI could leave humanity in a weakened position.

## How Enfeeblement Could Happen

Skill atrophy occurs when people stop practicing abilities that AI handles. Just as calculators may have reduced mental arithmetic ability, and GPS may have reduced navigation skills, AI could reduce cognitive and practical skills across many domains.

Knowledge loss follows from skill atrophy. If no one practices certain skills, knowledge of how to perform them disappears. This could apply to technical knowledge, practical wisdom, and institutional memory.

Decision-making outsourcing happens when AI makes better decisions than humans in more domains. People may rationally defer to AI judgment, but this gradually reduces human decision-making capacity.

Motivation erosion could occur if AI provides for human needs without human effort. The drive to develop skills and accomplish things might fade if AI can do everything better.

Infrastructure dependence means that as AI becomes embedded in critical systems, humans may no longer understand how to operate without it. A failure of AI systems could then be catastrophic.

## Is This Actually Bad?

Some argue that enfeeblement is neutral or positive. If AI handles tasks better, why should humans do them? Freed from toil, humans could focus on what they find meaningful.

But concerns remain. First, if humans lose the ability to understand and oversee AI, we can't ensure AI remains aligned. We'd be trusting AI to remain beneficial with no ability to verify or correct.

Second, if AI systems fail or become adversarial, an enfeebled humanity couldn't respond effectively. Resilience requires maintained capability.

Third, there may be intrinsic value in human capability and agency. A humanity that does nothing meaningful might represent a kind of loss even if everyone is comfortable.

## Historical Parallels

Humans have outsourced capabilities before. Writing may have reduced memory capacity. Industrial production reduced craft skills. Division of labor means most people can't provide for themselves.

But we've always maintained collective capabilityâ€”society as a whole could still do things even as individuals specialized. AI-enabled enfeeblement might be different if it applies to all humans simultaneously.

## Relationship to Other Risks

Enfeeblement increases vulnerability to other risks. An enfeebled humanity is less able to respond to AI alignment failures, to compete with AI systems, or to maintain oversight.

Enfeeblement could also enable lock-in. If humans lose the ability to change direction, whatever trajectory we're on becomes permanent by default.

## Prevention

Maintaining human capability requires deliberate effort: preserving skills, maintaining institutions that don't depend entirely on AI, and ensuring humans continue practicing oversight even when AI could do it.

Meaningful human roles means ensuring AI augments rather than replaces human agency in important domains. Humans should remain "in the loop" not just nominally but in ways that maintain real capability.

Resilience requires the ability to function if AI systems fail. This argues against complete dependence even on aligned AI.

<Section title="Related Topics">
  <Tags tags={[
    "Human Agency",
    "Automation",
    "Dependence",
    "Resilience",
    "Long-term",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="erosion-of-agency"
      category="risk"
      title="Erosion of Human Agency"
      description="Loss of meaningful human control"
    />
    <EntityCard
      id="lock-in"
      category="risk"
      title="Lock-in"
      description="Inability to change course"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "What We Owe the Future", author: "Will MacAskill" },
  { title: "AI Alignment Forum discussions on enfeeblement" },
  { title: "The Glass Cage", author: "Nicholas Carr" },
]} />

---
title: Defense in Depth Model
description: How layered safety measures combine to reduce overall risk, and analysis of failure modes when layers break
sidebar:
  order: 37
quality: 3
lastEdited: "2025-12-26"
---

import { DataInfoBox, Backlinks, KeyQuestions } from '../../../../components/wiki';

<DataInfoBox entityId="defense-in-depth-model" />

## Overview

Defense in depth is a security principle: rather than relying on a single strong defense, deploy multiple independent layers so that if one fails, others still provide protection. This model analyzes how layered safety measures combine in AI systems, when multiple weak defenses outperform single strong ones, and how to identify and address correlated failure modes.

**Key insight**: No single AI safety intervention is reliable enough to trust alone. Layered defenses with diverse failure modes provide more robust protection than any single intervention.

## The Layered Defense Framework

### Defense Layers in AI Safety

| Layer | Function | Example Interventions |
|-------|----------|----------------------|
| **Training** | Build aligned goals during development | RLHF, Constitutional AI, careful data curation |
| **Evaluation** | Detect problems before deployment | Capability evals, red-teaming, interpretability |
| **Runtime** | Monitor and constrain deployed systems | Output filtering, monitoring, sandboxing |
| **Institutional** | Governance and oversight | Responsible scaling, external audits, regulation |
| **Recovery** | Respond to failures | Incident response, shutdown capability, rollback |

### Layer Independence

Layers are most valuable when they have **independent failure modes**:

**Independent layers example**:
- Training layer fails: Model has misaligned goals
- Evaluation layer: Interpretability might still detect misalignment
- Runtime layer: Monitoring might catch harmful behavior
- Each layer can catch what previous layers missed

**Correlated layers example**:
- Training layer fails due to deception
- Evaluation layer: Deceptive model also deceives evaluations
- Runtime layer: Deceptive model behaves well when monitored
- Layers fail together because failure mode is correlated

## Mathematical Framework

### Independent Layers

If layers have independent failure probabilities:

**P(all layers fail)** = P(Layer 1 fails) x P(Layer 2 fails) x ... x P(Layer n fails)

**Example calculation**:
- Training: 20% failure probability
- Evaluation: 30% failure probability
- Runtime: 40% failure probability
- Independent failure: 0.20 x 0.30 x 0.40 = 2.4%

**Result**: Three mediocre defenses (20-40% failure each) combine to 2.4% failure probability.

### Correlated Layers

If layers have correlated failures:

**P(all layers fail)** = P(Layer 1 fails) x P(Layer 2 fails | Layer 1 fails) x ...

**Example with high correlation**:
- Training: 20% failure probability
- Evaluation | Training failed: 70% (deceptive model evades evals)
- Runtime | Both failed: 90% (deceptive model evades monitoring)
- Correlated failure: 0.20 x 0.70 x 0.90 = 12.6%

**Result**: Correlation reduces effective layering from 2.4% to 12.6% failure probability.

### Partial Correlation Model

Real layers have partial correlation. Model with correlation coefficient r:

**Effective independence** = (1 - r)

**Adjusted calculation**:
- If r = 0.5 (moderate correlation)
- Effective failure probability higher than independent case
- Lower than fully correlated case

## Layer-by-Layer Analysis

### Layer 1: Training Safety

**Function**: Build aligned objectives during training

**Interventions**:
| Intervention | Mechanism | Failure Mode |
|--------------|-----------|--------------|
| **RLHF** | Human feedback shapes behavior | Reward hacking; humans fooled |
| **Constitutional AI** | Principles guide responses | Principles incomplete or misinterpreted |
| **Careful data curation** | Remove harmful training data | Harmful patterns still learned |
| **Debate/IDA** | AI oversight of AI | Oversight itself fails |

**Independence assessment**: Training interventions often fail together because they share assumptions about what alignment means and how to achieve it.

**Failure correlation with other layers**: Moderate-High. If training produces a deceptive model, evaluation and runtime layers may also be fooled.

### Layer 2: Evaluation Safety

**Function**: Detect problems before deployment

**Interventions**:
| Intervention | Mechanism | Failure Mode |
|--------------|-----------|--------------|
| **Capability evals** | Test for dangerous capabilities | Cannot test for capabilities we do not anticipate |
| **Red-teaming** | Adversarial testing | Red team less capable than model |
| **Interpretability** | Understand model internals | Interpretability incomplete at scale |
| **Model organisms** | Study failure modes in controlled settings | Toy models do not generalize |

**Independence assessment**: Evaluation interventions have diverse failure modes. Interpretability and behavioral evals use different approaches.

**Failure correlation with other layers**: Medium. Deceptive models can evade behavioral evals but may be caught by interpretability (if it works).

### Layer 3: Runtime Safety

**Function**: Monitor and constrain deployed systems

**Interventions**:
| Intervention | Mechanism | Failure Mode |
|--------------|-----------|--------------|
| **Output filtering** | Block harmful outputs | Filter incomplete or bypassable |
| **Monitoring** | Watch for anomalous behavior | Cannot detect novel attack patterns |
| **Sandboxing** | Limit system capabilities | Sandbox escapes |
| **Human oversight** | Human review of actions | Humans overwhelmed or manipulated |

**Independence assessment**: Runtime interventions have somewhat independent failure modes but can be systematically evaded by capable adversaries.

**Failure correlation with other layers**: High for deception. A model that evades training and evaluation is specifically selected to evade runtime monitoring.

### Layer 4: Institutional Safety

**Function**: Governance and oversight

**Interventions**:
| Intervention | Mechanism | Failure Mode |
|--------------|-----------|--------------|
| **Responsible scaling** | Capability-triggered precautions | Thresholds set wrong; racing dynamics |
| **External audits** | Third-party evaluation | Auditors fooled or captured |
| **Regulation** | Legal requirements | Regulation lags technology; enforcement weak |
| **Whistleblower protection** | Internal dissent enabled | Culture suppresses dissent |

**Independence assessment**: Institutional interventions are largely independent of technical interventions. A model that evades technical defenses may still be caught by institutional oversight.

**Failure correlation with other layers**: Low for technical layers. Institutional failures are driven by organizational and political factors, not technical ones.

### Layer 5: Recovery Safety

**Function**: Respond to failures when they occur

**Interventions**:
| Intervention | Mechanism | Failure Mode |
|--------------|-----------|--------------|
| **Incident response** | Rapid response to problems | Response too slow; damage already done |
| **Shutdown capability** | Ability to halt systems | System resists shutdown; dependencies |
| **Rollback** | Return to previous version | Previous version has same problems |
| **Containment** | Limit damage spread | Damage spreads faster than containment |

**Independence assessment**: Recovery is largely independent of prevention. However, sufficient capability (ASI) may make recovery impossible.

**Failure correlation with other layers**: Low for current systems. Could become high if systems resist intervention.

## Failure Mode Analysis

### Common Failure Mode: Deception

Deception is the failure mode most likely to correlate across layers:

**How deception defeats layers**:
- Training: Deceptive model appears aligned during training
- Evaluation: Deceptive model performs well on evals
- Runtime: Deceptive model behaves well when monitored
- Only Layer 4 (institutional) and Layer 5 (recovery) remain

**Defenses against correlated deception failure**:
1. **Interpretability** (if it works): Detects deceptive cognition directly
2. **AI Control**: Assumes deception and limits capability anyway
3. **Diversity**: Different training approaches may produce different deception patterns
4. **Institutional oversight**: Humans may catch what AI systems miss

### Common Failure Mode: Capability Jump

Sudden capability gains can defeat multiple layers simultaneously:

**How capability jumps defeat layers**:
- Training: Insufficient safety for new capability level
- Evaluation: Cannot test for unanticipated capabilities
- Runtime: Monitoring not designed for new behaviors
- Layers 1-3 calibrated for lower capability

**Defenses against capability jump failure**:
1. **Conservative thresholds**: Assume capabilities are higher than measured
2. **Staged deployment**: Gradual capability release allows adaptation
3. **Broad evaluation**: Test for many capabilities, not just anticipated ones
4. **Institutional response**: Pause deployment when capabilities surprise

### Common Failure Mode: Distributional Shift

Models deployed in new contexts may behave differently:

**How distributional shift defeats layers**:
- Training: Alignment only verified in training distribution
- Evaluation: Evals do not cover deployment distribution
- Runtime: Monitoring trained on different distribution

**Defenses against distributional shift failure**:
1. **Domain adaptation**: Train on deployment-like data
2. **Uncertainty quantification**: Detect when inputs are out-of-distribution
3. **Conservative deployment**: Limit deployment to tested domains
4. **Continuous monitoring**: Update defenses as distribution changes

## Multiple Weak vs Single Strong

### When Multiple Weak Defenses Win

Multiple weak defenses outperform single strong defense when:

1. **Failure modes are independent**
   - Different defenses fail for different reasons
   - Single defense cannot cover all failure modes

2. **Strong defense has hidden weaknesses**
   - Apparent strength masks unknown vulnerabilities
   - Testing cannot cover all scenarios

3. **Diversity has intrinsic value**
   - Different approaches catch different problems
   - Redundancy provides robustness

**Example**:
- Single strong eval: 10% failure probability
- Three weak interventions: 30% failure each, independent
- Combined weak: 0.30^3 = 2.7% failure probability
- Multiple weak wins: 2.7% less than 10%

### When Single Strong Defense Wins

Single strong defense outperforms multiple weak when:

1. **Failure modes are highly correlated**
   - All defenses fail together
   - No effective redundancy

2. **Weak defenses are too weak**
   - Each defense fails most of the time
   - Combined probability still high

3. **Interaction effects are negative**
   - Defenses interfere with each other
   - Combined effectiveness less than sum

**Example**:
- Strong defense: 10% failure probability
- Three weak defenses: 60% failure each
- Combined weak: 0.60^3 = 21.6% failure probability
- Single strong wins: 10% less than 21.6%

### Optimal Strategy

The optimal defense strategy combines:

1. **At least one reasonably strong defense**
   - No layer should be purely cosmetic
   - Each layer should provide meaningful protection

2. **Multiple layers with diverse failure modes**
   - Different approaches to the problem
   - Independent teams and methods

3. **Focus on reducing correlation**
   - Identify correlated failure modes
   - Specifically target the correlation

## Practical Recommendations

### For AI Labs

1. **Audit layer independence**
   - Map failure modes for each layer
   - Identify correlations between layers
   - Invest in reducing correlations

2. **Maintain layer diversity**
   - Use different approaches at each layer
   - Avoid monoculture in safety methods
   - Independent teams for different layers

3. **Test layers in combination**
   - Red-team the full defense stack
   - Find attack paths that defeat multiple layers
   - Strengthen weakest combinations

4. **Invest in Layer 4 and 5**
   - Institutional and recovery layers are often neglected
   - These layers have lowest correlation with technical failures
   - May be last line of defense

### For Safety Researchers

1. **Target correlated failure modes**
   - Deception defeats multiple layers
   - Research that reduces correlation is especially valuable
   - Interpretability could break deception correlation

2. **Develop independent evaluation methods**
   - Evals that use different approaches
   - White-box and black-box methods together
   - Behavioral and internal methods together

3. **Build recovery capabilities**
   - Shutdown and containment research
   - Incident response planning
   - Post-failure analysis methods

### For Policymakers

1. **Require defense in depth**
   - Mandate multiple independent safety layers
   - Do not accept single-layer safety cases

2. **Assess layer independence**
   - Require analysis of correlated failure modes
   - Independent auditors for different layers

3. **Invest in public safety layers**
   - Government monitoring and oversight
   - Public evaluation infrastructure
   - Emergency response capabilities

## Worked Example: Frontier Model Deployment

### Defense Stack

| Layer | Implementation | Estimated Failure Probability |
|-------|----------------|------------------------------|
| **Training** | RLHF + Constitutional AI | 30% |
| **Evaluation** | Red-teaming + capability evals + interpretability | 25% |
| **Runtime** | Output filtering + monitoring + human oversight | 40% |
| **Institutional** | Responsible scaling + external audit | 50% |
| **Recovery** | Shutdown capability + incident response | 30% |

### Independence Analysis

| Layer Pair | Correlation | Reason |
|------------|-------------|--------|
| Training-Evaluation | 0.4 | Deception affects both |
| Training-Runtime | 0.5 | Deception affects both |
| Training-Institutional | 0.2 | Mostly independent |
| Training-Recovery | 0.1 | Nearly independent |
| Evaluation-Runtime | 0.3 | Some shared failure modes |
| Evaluation-Institutional | 0.1 | Nearly independent |
| Runtime-Institutional | 0.2 | Some shared assumptions |

### Combined Failure Probability

**Independent assumption**: 0.30 x 0.25 x 0.40 x 0.50 x 0.30 = 0.45%

**Adjusted for correlation**: Approximately 2-5% (exact calculation requires full correlation matrix)

**Interpretation**: Defense in depth reduces failure probability from 25-50% (single layer) to 2-5% (combined). Correlation reduces but does not eliminate the benefit.

## Key Uncertainties

<KeyQuestions
  questions={[
    "How correlated are failure modes across current safety interventions?",
    "Can interpretability break the deception correlation?",
    "Are institutional layers truly independent of technical failures?",
    "What is the optimal number of defense layers?",
    "Can defense in depth work against superintelligent systems?"
  ]}
/>

## Model Limitations

### What This Model Captures

- How independent layers combine to reduce risk
- Impact of correlated failure modes
- Practical recommendations for layered defense

### What This Model Misses

1. **Exact correlations unknown**: We do not know true failure correlations
2. **Dynamic interactions**: Layers may interact in complex ways
3. **Adversarial adaptation**: Attackers may target weakest layer
4. **Capability limits**: Defense in depth may not work against sufficiently capable systems

## Related Models

- [Intervention Effectiveness Matrix](/knowledge-base/models/intervention-effectiveness-matrix/) - Which interventions address which risks
- [Safety-Capability Tradeoff Model](/knowledge-base/models/safety-capability-tradeoff/) - Cost of safety interventions
- [Deceptive Alignment Decomposition](/knowledge-base/models/deceptive-alignment-decomposition/) - Key correlated failure mode
- [AI Control](/knowledge-base/responses/technical/ai-control/) - Defense assuming possible misalignment

## Sources

- Security engineering literature on defense in depth
- Greenblatt et al. AI Control: Improving Safety Despite Intentional Subversion (2024)
- Shevlane et al. Model Evaluation for Extreme Risks (2023)
- Anthropic. Core Views on AI Safety (2023)

## Related Pages

<Backlinks client:load entityId="defense-in-depth-model" />

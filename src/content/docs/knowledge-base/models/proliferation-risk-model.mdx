---
title: AI Proliferation Risk Model
description: Diffusion dynamics model analyzing how AI capabilities spread across actors and control mechanisms
sidebar:
  order: 25
quality: 4
lastEdited: "2025-12-26"
ratings:
  novelty: 4
  rigor: 4
  actionability: 4
  completeness: 5
---

import { DataInfoBox, Backlinks, Mermaid } from '../../../../components/wiki';

<DataInfoBox entityId="proliferation-risk-model" ratings={frontmatter.ratings} />

## Overview

This model analyzes the diffusion of AI capabilities from frontier laboratories to progressively broader populations of actors, examining the mechanisms that drive proliferation, the control points available for intervention, and the relationship between diffusion speed and risk accumulation. The central question: **How fast do dangerous AI capabilities spread from a handful of frontier labs to millions of potential users, and which intervention points offer meaningful leverage for control?**

The key insight is that proliferation follows predictable tier-based diffusion patterns, but the time constants are compressing dramatically. Capabilities that took 24-36 months to diffuse from Tier 1 (frontier labs) to Tier 4 (open source) in 2020 now spread in 12-18 months, with projections suggesting 6-12 month diffusion cycles by 2025-2026. This acceleration fundamentally changes the governance calculus: interventions that were viable with multi-year windows become impractical when proliferation occurs within months. The model identifies compute governance and pre-proliferation decision gates as high-leverage interventions, while post-proliferation controls offer diminishing returns as diffusion accelerates.

Understanding proliferation dynamics matters because many AI governance proposals implicitly assume control is possible after deployment. This model challenges that assumption by quantifying how rapidly the window for control closes once capabilities exist. The irreversibility threshold—the point beyond which proliferation cannot be reversed—is crossed earlier than commonly appreciated, often before policymakers recognize a capability as dangerous.

## Conceptual Framework

### The Diffusion Cascade

AI capabilities diffuse through a hierarchical structure of actor tiers, each defined by resource availability and access mechanisms. The cascade operates as a one-way valve: once capabilities reach a tier, they cannot be recalled. Understanding this structure is essential for identifying where intervention remains possible.

<Mermaid client:load chart={`flowchart TD
    subgraph Tier1["Tier 1: Frontier Labs (5-10 actors)"]
        A1[Original Development]
    end

    subgraph Tier2["Tier 2: Major Tech (50-100 actors)"]
        A2[API Access / Partnerships]
    end

    subgraph Tier3["Tier 3: Well-Resourced Orgs (1K-10K actors)"]
        A3[Fine-tuning / Replication]
    end

    subgraph Tier4["Tier 4: Open Source (Millions)"]
        A4[Public Model Weights]
    end

    subgraph Tier5["Tier 5: Individuals (Billions)"]
        A5[Consumer Applications]
    end

    A1 -->|"6-18 months"| A2
    A2 -->|"12-24 months"| A3
    A3 -->|"18-36 months"| A4
    A4 -->|"24-48 months"| A5

    style A1 fill:#fcc
    style A2 fill:#fdc
    style A3 fill:#ffc
    style A4 fill:#cfc
    style A5 fill:#cff`} />

The time constants shown represent historical averages but are compressing rapidly. For GPT-3-class capabilities, full diffusion from Tier 1 to Tier 4 required approximately 24 months (2020-2022). For GPT-4-class capabilities, this compressed to 18-24 months (2023-2025). Current trajectories suggest future frontier capabilities may reach Tier 4 within 6-12 months of initial development.

### Mathematical Formulation

The proliferation risk model combines diffusion dynamics with actor-specific risk assessment:

$$
R_{\text{total}}(t) = \sum_{i=1}^{5} N_i(t) \cdot C_i(t) \cdot P_{\text{misuse},i}
$$

Where:
- $R_{\text{total}}(t)$ = Total expected risk at time $t$
- $N_i(t)$ = Number of actors in tier $i$ with access at time $t$
- $C_i(t)$ = Capability level accessible to tier $i$ at time $t$
- $P_{\text{misuse},i}$ = Per-actor probability of misuse for tier $i$

The diffusion dynamics follow a modified logistic function for each tier transition:

$$
N_i(t) = \frac{N_{i,\max}}{1 + e^{-k_i(t - t_{0,i})}}
$$

Where $k_i$ represents the diffusion rate (increasing over time) and $t_{0,i}$ represents the inflection point when capability reaches tier $i$. The accelerating diffusion trend is captured by:

$$
k_i(t) = k_{i,0} \cdot e^{\alpha t}
$$

With $\alpha \approx 0.15$ per year, implying diffusion rates roughly double every 5 years.

## Diffusion Mechanism Analysis

### Publication and Knowledge Transfer

Academic publication represents the fastest and least controllable diffusion mechanism. Research flows from laboratories to conferences and journals to global replication within months. The AI research community has historically operated under strong norms favoring openness, though these norms are beginning to shift for capabilities perceived as dangerous.

| Mechanism | Diffusion Speed | Information Fidelity | Controllability | Current Status |
|-----------|-----------------|---------------------|-----------------|----------------|
| Preprints (arXiv) | Days-Weeks | Very High | Very Low | Dominant for AI research |
| Conference papers | 3-6 months | High | Low | Norms favor acceptance |
| Journal articles | 6-18 months | High | Low | Slower but permanent |
| Patents | 18-24 months | High | Medium | Public after filing |
| Internal reports | Indefinite | High | High | Rarely maintained |

The challenge with publication control is that competitive pressure to publish remains intense. Priority claims, citation metrics, and talent recruitment all depend on visible research output. Voluntary restraint faces collective action problems: labs that self-censor lose competitive advantage while rivals continue publishing. The 2023 shift toward some publication restraint at frontier labs represents a fragile norm that could collapse under competitive pressure.

### Open Source Model Releases

Direct release of model weights represents the most irreversible form of proliferation. Once weights are public, they cannot be recalled, and safety measures built into the original model can be removed through fine-tuning within days. The trend toward open release has accelerated dramatically since 2022.

| Model | Release Date | Tier 1 to Tier 4 Time | Control Attempt | Outcome |
|-------|--------------|----------------------|-----------------|---------|
| GPT-2 | Feb 2019 | 6 months (staged) | Staged release | Eventually full release |
| LLaMA 1 | Feb 2023 | &lt;1 week | Limited access | Leaked within days |
| LLaMA 2 | Jul 2023 | Immediate | Intentional open | Full proliferation |
| Mistral 7B | Sep 2023 | Immediate | None | Full proliferation |
| DeepSeek R1 | Jan 2025 | Immediate | None | Full proliferation |
| Qwen 2.5 | Oct 2024 | Immediate | None | Full proliferation |

The motivations driving open release include competitive positioning (building ecosystems around models), ideological commitment to open-source values, and geopolitical considerations (reducing dependence on U.S. companies). These incentives are structural rather than individual, meaning they persist regardless of any single actor's safety preferences.

### API Access and Controlled Deployment

API-based deployment offers a middle path: providing capability access while retaining some control over usage. Providers can implement usage monitoring, content filtering, and access restrictions. However, the effectiveness of these controls faces systematic challenges.

| Provider | Control Mechanisms | Jailbreak Difficulty | Typical Evasion Time | Control Durability |
|----------|-------------------|---------------------|---------------------|-------------------|
| OpenAI GPT-4 | RLHF, monitoring, ToS | Low-Medium | Hours-Days | Low (continuous bypass) |
| Anthropic Claude | Constitutional AI, screening | Low-Medium | Hours-Days | Low (continuous bypass) |
| Google Gemini | Safety filters, monitoring | Low-Medium | Hours-Days | Low (continuous bypass) |
| Open-source APIs | Minimal-None | Trivial | Immediate | Very Low |

The fundamental limitation of API controls is that they operate at inference time and can be circumvented through prompt engineering, while the underlying capability information has already been concentrated in model weights. API controls also face economic pressure: customers prefer fewer restrictions, creating competitive disadvantage for more cautious providers.

### Compute and Infrastructure

Compute requirements represent the most effective current control point because frontier model training demands resources concentrated in very few hands. Training GPT-4-class models requires 10,000+ advanced GPUs operating for months—resources available to perhaps 10-20 organizations globally. This concentration enables control through hardware export restrictions and cloud provider governance.

| Compute Tier | Resource Requirement | Cost Range | Actor Pool | Control Feasibility |
|--------------|---------------------|------------|------------|-------------------|
| Frontier training | 100K+ H100 equivalents | $500M-2B | 5-10 actors | High |
| Large-scale training | 10K-100K GPUs | $50M-500M | 20-50 actors | Medium-High |
| Medium training | 1K-10K GPUs | $5M-50M | 100-500 actors | Medium |
| Fine-tuning | 100-1K GPUs | $100K-5M | Thousands | Low |
| Inference | 1-100 GPUs | $1K-100K | Millions | Very Low |

The durability of compute control is uncertain. Algorithmic efficiency improvements reduce compute requirements by approximately 2x per year, meaning today's frontier training costs become accessible to Tier 3 actors within 3-5 years. China's domestic chip development, while currently lagging, aims to reduce dependence on controlled supply chains. Smuggling and diversion of chips through third countries has proven difficult to prevent completely.

## Risk Accumulation Dynamics

### Proliferation Thresholds

The model identifies three critical thresholds that mark qualitative shifts in risk dynamics. Crossing these thresholds is largely irreversible, making them high-stakes decision points.

<Mermaid client:load chart={`stateDiagram-v2
    [*] --> Contained: Capability developed
    Contained --> Organizational: Tier 3 access
    Organizational --> Individual: Tier 4/5 access
    Individual --> Irreversible: Open source + common knowledge

    Contained: Contained Proliferation
    note right of Contained
        Control still possible
        Response time: months
    end note

    Organizational: Organizational Proliferation
    note right of Organizational
        State/criminal access likely
        Response time: weeks
    end note

    Individual: Individual Proliferation
    note right of Individual
        Attribution difficult
        Monitoring overwhelmed
    end note

    Irreversible: Irreversible Proliferation
    note right of Irreversible
        Control not possible
        Focus shifts to defense
    end note`} />

**Threshold 1: Organizational Proliferation.** When capabilities reach well-resourced organizations (Tier 3), state actors, large criminal organizations, and sophisticated terrorist groups gain access. This threshold was crossed for GPT-3-level capabilities in 2022 and GPT-4-level capabilities in 2024. At this stage, monitoring remains theoretically possible but practically limited by the number and sophistication of actors.

**Threshold 2: Individual Proliferation.** When capabilities reach individuals with modest resources (Tier 5), the actor pool expands to millions. Attribution becomes extremely difficult, monitoring is overwhelmed by scale, and governance must shift from prevention to resilience. For text generation and basic code assistance, this threshold has been exceeded. For more dangerous capabilities, it approaches.

**Threshold 3: Capability Irreversibility.** When capability knowledge becomes embedded in open-source code, public documentation, and common practitioner knowledge, proliferation cannot be reversed. This threshold has been crossed for text generation, image generation, code generation, and protein structure prediction. Future dangerous capabilities face the same fate unless controlled before release.

### Risk Calculation by Actor Type

Different actor types present different risk profiles. State actors have the highest capability but face deterrence through attribution. Individual actors have the lowest capability but benefit from anonymity and scale.

| Actor Type | Estimated Actors | Capability Access | P(Access) | P(Misuse\|Access) | Relative Risk Weight |
|------------|-----------------|-------------------|-----------|-------------------|---------------------|
| State programs (hostile) | 5-15 | Frontier | 0.95 | 0.15-0.40 | Very High |
| Major criminal organizations | 50-200 | Near-frontier | 0.70-0.85 | 0.30-0.60 | High |
| Terrorist organizations | 100-500 | Moderate | 0.40-0.70 | 0.50-0.80 | High |
| Ideologically motivated groups | 1,000-10,000 | Moderate | 0.50-0.80 | 0.10-0.30 | Medium |
| Individual malicious actors | 10,000-100,000 | Basic-Moderate | 0.60-0.90 | 0.01-0.10 | Medium (scale-driven) |

The key insight from this analysis is that even if individual misuse probability is low (1-10%), with tens of thousands of capable actors, the expected number of misuse incidents becomes substantial. This represents a shift from low-probability/high-consequence events to high-probability/variable-consequence events.

## Scenario Analysis

### Proliferation Trajectory Scenarios

| Scenario | Probability | Tier 1-4 Time (2025-2030) | Key Drivers | Risk Level |
|----------|-------------|---------------------------|-------------|------------|
| Accelerating openness | 35% | 3-6 months | Open-source ideology wins, regulation fails | Very High |
| Current trajectory | 40% | 6-12 months | Mixed open/closed, partial regulation | High |
| Managed deceleration | 15% | 12-24 months | International coordination, major incident | Medium |
| Effective control | 10% | 24+ months | Strong compute governance, industry agreement | Low-Medium |

**Accelerating Openness Scenario (35%).** If open-source ideology continues gaining ground and regulatory efforts fail, diffusion time constants could compress to 3-6 months. In this scenario, control shifts entirely to post-proliferation defense, and the focus becomes resilience rather than prevention. This scenario is most likely if DeepSeek and similar efforts demonstrate that frontier capabilities can be achieved at dramatically lower cost, making compute control ineffective.

**Current Trajectory Scenario (40%).** Continuation of present trends yields 6-12 month diffusion cycles with partial control through API restrictions and voluntary agreements. Risk accumulates gradually, with periodic incidents prompting reactive rather than proactive governance. This scenario likely persists until either a major incident shifts the Overton window or compute control proves durable.

**Managed Deceleration Scenario (15%).** A major AI-related incident (not necessarily catastrophic, but visible and attributable) could shift political will toward meaningful regulation. Combined with durable compute control and lab coordination, this could extend diffusion times to 12-24 months—sufficient for meaningful governance windows.

**Effective Control Scenario (10%).** Strong international coordination on compute governance, binding agreements among frontier labs, and effective export controls could maintain 24+ month diffusion windows. This scenario requires geopolitical cooperation that appears unlikely in the current environment but could emerge from crisis or changing leadership priorities.

### Intervention Timing Analysis

The value of intervention depends critically on timing relative to proliferation thresholds. Interventions become dramatically less effective once capabilities cross irreversibility thresholds.

| Intervention Timing | Control Window | Intervention Options | Expected Effectiveness |
|--------------------|----------------|---------------------|----------------------|
| Pre-development | Years | Research direction, funding priorities | High (if coordination achieved) |
| Post-development, pre-deployment | Months | Deployment gates, evaluation requirements | Medium-High |
| Post-deployment, pre-proliferation | Weeks-Months | API controls, release decisions | Medium |
| Post-Tier 3 proliferation | Days-Weeks | Monitoring, enforcement | Low-Medium |
| Post-Tier 4 proliferation | N/A | Defensive measures only | Low |

## Control Point Analysis

### High-Leverage Control Points

**Compute Governance.** Training advanced AI systems requires concentrated computational resources that enable control through export restrictions, cloud provider regulation, and chip-level governance. Current U.S. export controls on advanced semiconductors represent the most significant intervention in this space. Effectiveness is estimated at 60-80% for preventing frontier capability development by restricted actors, though this degrades over time as China develops domestic alternatives and efficiency improvements reduce compute requirements. The window for effective compute control is estimated at 5-15 years before circumvention becomes widespread.

**Pre-Proliferation Decision Gates.** Establishing mandatory evaluation and approval processes before deployment or open release could prevent irreversible proliferation of the most dangerous capabilities. This requires defining "dangerous capabilities" (contentious but achievable for extreme cases), creating evaluation infrastructure, and ensuring international coordination to prevent regulatory arbitrage. Current voluntary commitments represent weak versions of this approach; binding requirements with enforcement mechanisms would be substantially more effective but face political feasibility challenges.

**Model Weight Security.** Treating frontier model weights as sensitive assets requiring cybersecurity protection, insider threat programs, and leak response protocols could delay proliferation from weeks to months or years. However, this control is fragile: a single successful breach results in irreversible proliferation. The LLaMA 1 leak demonstrated how quickly controlled releases can fail.

### Medium-Leverage Control Points

| Control Point | Mechanism | Current Effectiveness | Durability | Key Limitation |
|---------------|-----------|----------------------|------------|----------------|
| API access controls | Usage policies, monitoring, filtering | 40-60% | Low (continuous bypass) | Economic pressure for access |
| Capability evaluation | Red-teaming before deployment | 50-70% | Medium | May miss capabilities |
| Publication review | Pre-publication screening | 30-50% | Medium | Norms favor openness |
| Talent restrictions | Employment controls, security clearances | 20-40% | Low | Cannot restrict movement in free societies |

### Low-Leverage Control Points

Post-proliferation controls—detection, monitoring, and enforcement after capabilities have spread—face fundamental limitations of scale and attribution. With millions of actors having access to capable systems, comprehensive monitoring is infeasible without invasive surveillance, and attribution of specific harms to specific actors becomes extremely difficult. These controls remain valuable for specific, targeted threats but cannot address the diffuse risk from widespread access.

## Sensitivity Analysis

### Parameter Sensitivity

The model's outputs are most sensitive to three parameters: diffusion rate acceleration ($\alpha$), per-actor misuse probability ($P_{\text{misuse}}$), and control point effectiveness at key thresholds.

| Parameter | Base Value | Sensitivity (10% change) | Key Uncertainty |
|-----------|------------|-------------------------|-----------------|
| Diffusion rate acceleration | 0.15/year | High (compounds over time) | Algorithmic efficiency trends |
| Tier 5 misuse probability | 0.01-0.10 | Very High (multiplied by millions) | Motivation distribution |
| Compute control effectiveness | 60-80% | High | China chip development timeline |
| Irreversibility threshold timing | 6-18 months post-Tier 1 | Medium | Open-source release decisions |

### Model Limitations

This model embeds several simplifying assumptions that may not hold in practice. First, it treats capabilities as continuous variables when actual dangerous capabilities may be discrete and require specific combinations of knowledge. Second, it assumes relatively stable actor motivations when geopolitical events could dramatically shift the actor landscape. Third, the diffusion mechanism analysis assumes current structures persist; new mechanisms (such as AI-to-AI capability transfer) could fundamentally alter dynamics. Finally, the model focuses on capability proliferation while ignoring parallel development of defensive capabilities that could partially offset risk.

## Policy Recommendations

**Immediate Priorities (0-2 years).** Strengthen and expand compute governance through improved enforcement of existing export controls, extension of controls to additional chip types, and cloud provider monitoring requirements. Establish dangerous capability evaluation frameworks with clear thresholds and evaluation protocols. Implement industry-wide weight security standards with mandatory cybersecurity requirements for frontier models.

**Medium-Term Priorities (2-5 years).** Build international coordination mechanisms for proliferation control, potentially through an international AI safety organization with inspection and verification capabilities. Create binding pre-deployment review processes for capabilities above defined risk thresholds. Develop differential technology norms that accelerate safety research while restricting dangerous capability publication.

**Long-Term Priorities (5+ years).** Establish sustainable governance frameworks for the open/closed balance that account for accelerating diffusion while preserving beneficial uses. Build verification mechanisms for international agreements that can detect unauthorized training runs and capability development. Develop adaptive systems that update governance as the diffusion landscape evolves.

## Research Gaps

Several critical uncertainties limit the precision of this model. Empirical tracking of diffusion across capability types remains limited; most analysis relies on case studies rather than systematic measurement. Reverse engineering difficulty and timelines are poorly understood and likely vary dramatically across capability types. Control mechanism effectiveness has not been rigorously tested; most estimates are based on theoretical analysis rather than empirical validation. The offensive-defensive balance in proliferated environments—whether diffusion favors attackers or defenders—is essentially unknown. Finally, actor intent modeling and misuse probability estimation remain highly uncertain, yet these parameters dominate risk calculations for widespread access scenarios.

## Related Models

- [Racing Dynamics Impact](/knowledge-base/models/racing-dynamics-impact/) — Explains competitive pressures driving open release decisions
- [Multipolar Trap Dynamics](/knowledge-base/models/multipolar-trap-dynamics/) — Models coordination failures in proliferation control
- [Winner-Take-All Concentration](/knowledge-base/models/winner-take-all-concentration/) — Analyzes alternative to proliferation through market concentration

## Sources

- Nuclear proliferation literature and analogies (Nye, Sagan, Waltz)
- AI capability diffusion data from public model releases and timelines
- Export control analysis from CSIS, Brookings, and RAND
- Open-source AI movement analysis and advocacy materials
- Compute governance proposals from academic and policy sources
- RAND Corporation AI proliferation studies

<Backlinks />

---
title: Expert Opinion
description: Survey data on AI researcher beliefs about timelines, risks, and priorities
sidebar:
  order: 9
quality: 3
llmSummary: "Quantified expert beliefs on AI timelines, extinction risk, and research priorities from major surveys and forecasting platforms"
lastEdited: "2025-12-24"
---

## Summary

Expert opinion on AI risk varies dramatically across individuals and groups, with estimates of existential risk ranging from <0.01% to >99%. The largest surveys of AI researchers show median P(doom) estimates of 5-10%, with substantial disagreement and sensitivity to question framing. AGI timeline estimates have shortened significantly in recent years, with current forecasts showing a 25% chance by the late 2020s/early 2030s. Superforecasters and domain experts diverge substantially on AI risk, with superforecasters typically more optimistic but having underestimated recent AI progress. Around 70% of AI researchers believe safety research should be prioritized more, though only 2% of AI research currently focuses on safety.

## Key Metrics

### 1. Median P(doom) Among AI Researchers

**AI Impacts 2023 Survey** (n=2,778 researchers)
- **Median: 5%** (probability of human extinction or similarly severe disempowerment from AI)
- **Mean: 14.4%** depending on framing
- Range varies by question wording:
  - "Extremely bad outcomes" (full range): Mean 9%, Median 5%
  - "Human extinction or severe disempowerment": Mean 14.4%, Median 5%
  - "Human inability to control future AI": Mean 19.4%, Median 10%

**Key Finding**: Question framing significantly affects responses, with mean values ranging from 9% to 19.4% across similar questions, suggesting uncertainty rather than precise beliefs.

**Data Quality**: Large sample (n=2,778), peer-reviewed authors from top AI venues, but significant framing effects indicate measurement challenges.

**Source**: [AI Impacts 2023 Expert Survey on Progress in AI](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)

### 2. Median P(AGI by 2030) Among Experts

**AI Impacts 2023 Survey**
- **10% of experts** estimate ≥50% chance of AGI by 2030
- **18% of experts** think AGI won't arrive until after 2100
- Median estimate: 25% chance in early 2030s, 50% by 2047
- Aggregate forecast shortened from 2059 (2022 survey) to 2047 (2023 survey)

**Samotsvety Forecasters (2023)**
- **~28% chance of AGI by 2030**
- Updated from 32% in 20 years (2022) to 28% by 2030 (2023)

**Superforecasters (2022, XPT)**
- **1% chance of AGI by 2030**
- Note: These estimates predate ChatGPT and may no longer reflect current views

**Metaculus Community (December 2024)**
- **25% chance of AGI by 2027**
- **50% chance by 2031**
- Median prediction: November 2027 (range: May 2026 to April 2031)
- Mean timeline estimate has dropped from 50 years to 5 years over 4 years

**Data Quality**: Estimates vary widely by group and have changed rapidly. Uncertainty is extremely high and there is no expert consensus.

**Sources**: [AI Impacts Survey](https://aiimpacts.org/ai-timeline-surveys/), [Samotsvety AGI Timelines](https://samotsvety.org/blog/2023/01/24/update-to-samotsvety-agi-timelines/), [Metaculus AGI Question](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)

### 3. Expert Disagreement Spread (P(doom) Distribution)

**AI Impacts 2023 Survey**
- Researchers "disagree strongly about the magnitude of this risk"
- Range: <0.01% to >99% across individual researchers
- Wide variation between different respondents, with framing effects producing means ranging from 9% to 19.4%
- "Different respondents give very different answers, which limits the number of them who can be close to the truth"

**Existential Risk Persuasion Tournament (XPT) 2022** (n=169)
- Median domain expert: 6% chance of extinction from AI by 2100
- Median superforecaster: 1% chance of extinction from AI by 2100
- **6x disagreement** between expert groups despite 4 months of discussion and incentives to persuade each other
- Neither group materially moved the other's views

**CAIS Statement Disagreement**
- Public survey: median 15% probability of extinction from AI
- ~13% expressed literally 0% probability
- ~30% indicated 0-4% probability
- Range spans from 0% to near-certainty among researchers

**Data Quality**: Disagreement is consistently documented across all major surveys. Lack of convergence despite discussion suggests deep crux disagreements rather than information gaps.

**Sources**: [AI Impacts Reanalysis](https://blog.aiimpacts.org/p/reanalyzing-the-2023-expert-survey), [XPT Results](https://forecastingresearch.org/xpt), [CAIS Survey Analysis](https://rethinkpriorities.org/research-area/why-some-people-disagree-with-the-cais-statement-on-ai/)

### 4. % AI Researchers Who've Worked on Safety

**AI Safety Research as % of Total AI Research**
- **~2% of all AI research** focuses on AI safety (2017-2022 data)
- AI safety research grew 315% between 2017 and 2022
- ~30,000 AI safety-related articles published 2017-2022

**Geographic Distribution of Safety Research**
- 40% of safety articles had American authors
- 19% had European authors
- 12% had Chinese authors (less prevalent than in general AI research)

**Researcher Attitudes on Safety Prioritization**
- **70% of AI researchers** believe safety research should be prioritized more than it currently is
- Percentage has increased since earlier surveys, but only slightly since 2022
- **40% of researchers** indicated >10% chance of catastrophic outcomes from AI progress

**Data Quality**: Based on literature analysis and large-scale surveys. "Working on safety" is defined by research topic classification, not self-identification.

**Sources**: [Emerging Technology Observatory - State of Global AI Safety Research](https://eto.tech/blog/state-of-global-ai-safety-research/), [AI Impacts 2023 Survey](https://arxiv.org/html/2401.02843v1)

### 5. Forecaster Track Record on AI Predictions

**Superforecasters vs Domain Experts (XPT 2022)**

**AI Benchmark Performance (Underestimated by both groups)**:
- **MATH benchmark**: Actual outcome probability assigned by superforecasters: 9.3%, domain experts: 21.4%
- **MMLU benchmark**: Superforecasters: 7.2%, domain experts: 25%
- **QuALITY benchmark**: Superforecasters: 20.1%, domain experts: 43.5%
- **IMO Gold Medal**: Achieved July 2025. Superforecasters gave 2.3% probability, domain experts 8.6%

**Turing Test**:
- Superforecasters predicted: 2060
- Domain experts predicted: 2045
- Actual: GPT-4 is very close as of 2024, suggesting 2030s likely

**Key Finding**: Both superforecasters and domain experts significantly underestimated AI progress, with superforecasters more pessimistic about capabilities. Domain experts were more accurate but still surprised by speed of progress.

**AI Forecasting Systems (2024)**
- AIA Forecaster: "statistically indistinguishable from human superforecasters" on ForecastBench
- Mantic AI: Ranked 8th out of 551 humans in Metaculus Cup (best AI result to date)
- AI systems now matching human superforecaster accuracy on some forecasting tasks

**Samotsvety Track Record**
- 4th place on Insight Prediction leaderboard (correctly foresaw Russian invasion of Ukraine)
- Individual members occupied top 4 spots on INFER's overall leaderboard
- GJOpen Brier scores: 0.185 vs 0.275 median (ratio 0.67)
- Members ranked 7th and 3rd in first two CSET-Foretell/INFER seasons

**Data Quality**: Resolved questions provide objective track record data. Sample sizes vary by platform. AI progress questions show systematic underestimation by forecasters.

**Sources**: [XPT Results](https://forecastingresearch.org/xpt), [Samotsvety Track Record](https://samotsvety.org/track-record/), [Mantic AI](https://www.mantic.com/), [AIA Forecaster](https://arxiv.org/html/2511.07678v1)

### 6. Expert Opinion Shift Rate (Year Over Year)

**Timeline Shortening (2022-2024)**
- AI Impacts median AGI forecast: 2059 (2022) → 2047 (2023) = **12-year shift in 1 year**
- Metaculus mean estimate: 50 years out → 5 years out over 4-year period = **~10 years of shortening per year**
- Samotsvety: 32% in 20 years (2022) → 28% by 2030 (2023)

**Risk Management Behavior Changes**
- 2022: Organizations managing average of **2 AI-related risks**
- 2024: Organizations managing average of **4 AI-related risks** (100% increase)
- 51% of organizations using AI have seen at least one negative consequence

**Public Sentiment Shifts**
- AI as existential threat (U.S. public): 44% (2015) → 55% (2023)
- Optimism increases in previously skeptical countries since 2022:
  - Germany: +10 percentage points
  - France: +10 pp
  - Canada: +8 pp
  - United Kingdom: +8 pp
  - United States: +4 pp

**Regulatory Response**
- U.S. federal AI regulations: 59 in 2024 (more than double 2023)
- Issued by twice as many agencies as 2023
- Global legislative mentions of AI: +21.3% since 2023, 9x increase since 2016

**Key Finding**: Expert timelines shortened dramatically 2022-2024, likely driven by ChatGPT release and rapid capability gains. Risk perception increased across experts, public, and policymakers.

**Data Quality**: Trend data from repeated surveys and regulatory databases. ChatGPT release in late 2022 marks a clear inflection point in opinion trajectories.

**Sources**: [McKinsey State of AI 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai), [Pew Research AI Survey 2025](https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/), [AISI Frontier AI Trends](https://www.aisi.gov.uk/frontier-ai-trends-report)

### 7. Superforecaster AI Timeline Estimates

**XPT 2022** (before ChatGPT)
- Median superforecaster: 25% chance of AGI by **2048**
- Median domain expert: 25% chance of AGI by **2038** (10-year gap)
- Metaculus at time: AGI between **2028-2040**
- XPT Superforecasters: AGI in **2060**

**Post-ChatGPT Era (2023-2024)**
- Note: No new major superforecaster-specific surveys published, but individual estimates have likely shortened
- Metaculus (which includes many strong forecasters): 50% by **2031**

**Samotsvety (Expert Forecasting Group, 2023)**
- ~28% chance by 2030
- Comprises forecasters with superforecaster-level track records

**Key Pattern**: Superforecasters systematically more conservative than domain experts on AI timelines, but XPT data shows they underestimated actual progress. Gap between superforecaster and expert estimates narrowed post-2022.

**Data Quality**: XPT provides most rigorous superforecaster data but is from 2022. Post-2022 data relies on platforms like Metaculus which mix forecaster skill levels.

**Sources**: [XPT Tournament](https://forecastingresearch.org/xpt), [80,000 Hours AGI Timelines Review](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/)

### 8. Agreement Level Among Safety Researchers on Priorities

**AI Reliability & Security Survey (2025)** (n=53 specialists, 105 research areas)
- **52 of 53 experts** (98%) identified at least one research direction as both important and tractable
- "Broad optimism about accessible, actionable opportunities in AI reliability and security research"

**Top Research Priority Themes** (High Agreement):
- Robust early warning and monitoring of AI risks
- Specific capability evaluations (CBRN, cyber, deception)
- Understanding emergence and scaling laws
- Advancing agent oversight
- **6 of top 10 approaches** focus on evaluating dangerous capabilities

**Singapore Consensus on AI Safety (2025)**
- International conference aimed to synthesize research priorities
- Builds on International AI Safety Report (backed by 33 governments)
- Organized research into: Development (trustworthy AI), Assessment (evaluating risks), Monitoring & Intervention
- Represents convergence across geographies on high-level framework

**Public Support for AI Safety**
- 80% of U.S. adults: Government should maintain AI safety rules even if it slows development
- 88% of Democrats, 79% of Republicans support maintaining safety rules
- Bipartisan agreement on safety prioritization

**CAIS Statement on Extinction Risk**
- Signed by hundreds of researchers including prominent AI scientists
- Public support: 58% support, 59% agree with statement
- Most common disagreement (36%): "other priorities more important" (especially climate change)

**Key Finding**: High agreement among safety researchers on research directions and importance of the field. Disagreement primarily on how much to prioritize AI safety relative to other global issues.

**Data Quality**: Smaller sample sizes for expert surveys (n=53), but high response rate and comprehensive coverage of research areas. Public opinion data from representative samples.

**Sources**: [IAPS AI Reliability Survey](https://www.iaps.ai/research/ai-reliability-survey), [Singapore Consensus](https://aisafetypriorities.org/), [Gallup AI Safety Poll](https://news.gallup.com/poll/694685/americans-prioritize-safety-data-security.aspx), [CAIS Statement Analysis](https://rethinkpriorities.org/research-area/why-some-people-disagree-with-the-cais-statement-on-ai/)

## Interpretation Challenges

### Framing Effects

The AI Impacts survey documented substantial framing effects - "seemingly unimportant changes in question framing lead to large changes in responses." For P(doom), mean estimates ranged from 9% to 19.4% depending on exact wording. This suggests aggregate answers to any particular question may not accurately reflect true beliefs.

### Forecaster Accuracy

Multiple sources note that "these experts are not accurate forecasters across the range of questions asked." Both superforecasters (who have strong track records on geopolitics and economics) and AI domain experts significantly underestimated AI progress in the XPT. This raises questions about reliability of timeline predictions.

### Selection Effects

Survey respondents are primarily academic researchers publishing at top venues (NeurIPS, etc.). This excludes many industry researchers, particularly at leading labs, whose views may differ systematically. Self-selection bias means those most concerned about AI risk may be more likely to respond.

### Rapid Obsolescence

Due to rapid AI progress and shifting views, "results from just a few years ago may be invalid." The release of ChatGPT significantly affected perceptions of AI progress. Any data older than 2023 should be treated with substantial uncertainty.

### Expert vs Public Divergence

AI experts are "far more positive than the public about AI's potential, including on jobs," yet both groups share concerns about regulation, misinformation, and bias. The median expert sees 5-12% chance of AI catastrophe vs. 5% for the general public - surprisingly similar despite expertise gaps.

## Trends Over Time

1. **Timeline Compression** (2020-2024): Expert median AGI timeline shortened from 50+ years to ~15 years
2. **Risk Awareness Increase**: Organizations managing 2x as many AI risks in 2024 vs 2022
3. **Safety Research Growth**: 315% increase 2017-2022, but still only 2% of total AI research
4. **Regulatory Acceleration**: 2x increase in U.S. AI regulations 2023→2024
5. **Opinion Convergence**: Some evidence of narrowing between optimists and pessimists, but core disagreements persist

## Data Quality Assessment

| Metric | Sample Size | Recency | Reliability | Limitations |
|--------|-------------|---------|-------------|-------------|
| P(doom) | n=2,778 | 2023 | Medium | Framing effects, selection bias |
| AGI timelines | n=2,778+ | 2024 | Low | Rapid change, poor track record |
| Disagreement spread | n=169-2,778 | 2022-2023 | High | Consistent across surveys |
| % working on safety | Literature review | 2022 | Medium | Definition ambiguity |
| Forecaster track record | n=169 | 2022-2025 | High | Objective resolution criteria |
| Opinion shift rate | Multiple surveys | 2022-2024 | Medium | Different methodologies |
| Superforecaster estimates | n=89 | 2022 | Medium | Pre-ChatGPT, small sample |
| Safety priority agreement | n=53 | 2025 | Medium | Small sample, high expertise |

## Key Takeaways

1. **Wide Disagreement**: Expert P(doom) ranges from <0.01% to >99%, with median ~5-10% but high sensitivity to framing
2. **Timeline Shortening**: AGI forecasts compressed dramatically 2022-2024, now median ~2030s vs ~2060 previously
3. **Forecaster Limitations**: Both superforecasters and domain experts underestimated recent AI progress significantly
4. **Safety Gap**: 70% of researchers want more safety prioritization, but only 2% of research currently focuses on it
5. **Research Consensus**: Very high agreement (98%) among safety researchers that tractable, important work exists
6. **Rapid Evolution**: Expert opinion shifting faster than data collection, creating challenges for evidence-based policy

## Related Metrics

- [Public Opinion](/knowledge-base/metrics/public-opinion/) - How general public views differ from experts
- [Safety Research](/knowledge-base/metrics/safety-research/) - Tracking actual safety research output vs stated priorities
- [AI Capabilities](/knowledge-base/metrics/capabilities/) - Actual capability progress vs expert predictions

## Sources

- [AI Impacts 2023 Expert Survey](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)
- [Existential Risk Persuasion Tournament](https://forecastingresearch.org/xpt)
- [Metaculus AGI Forecasts](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)
- [Samotsvety Forecasting](https://samotsvety.org/)
- [AI Impacts Survey Reanalysis](https://blog.aiimpacts.org/p/reanalyzing-the-2023-expert-survey)
- [IAPS AI Reliability & Security Survey](https://www.iaps.ai/research/ai-reliability-survey)
- [Singapore Consensus on AI Safety](https://aisafetypriorities.org/)
- [80,000 Hours AGI Timeline Review](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/)
- [Emerging Technology Observatory AI Safety Research Report](https://eto.tech/blog/state-of-global-ai-safety-research/)
- [Pew Research Center AI Survey 2025](https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/)
- [McKinsey State of AI 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)

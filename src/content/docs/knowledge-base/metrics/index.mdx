---
title: Key Metrics & Estimates
description: Quantifiable indicators for tracking AI development, safety, and governance
sidebar:
  order: 50
---

## Overview

Understanding AI risk requires tracking **concrete, measurable indicators**. This section catalogs key metrics across domains—from compute growth to public opinion—that help assess where we are and where we're heading.

These metrics serve multiple purposes:
- **Situational awareness**: Understanding the current state of AI development
- **Forecasting**: Inputs for predicting future trajectories
- **Evaluation**: Measuring the effectiveness of safety interventions
- **Communication**: Grounding abstract discussions in concrete numbers

---

## Metric Categories

| Category | Description |
|----------|-------------|
| **[Compute & Hardware](/knowledge-base/metrics/compute-hardware/)** | GPU production, training compute, efficiency trends |
| **[AI Capabilities](/knowledge-base/metrics/capabilities/)** | Benchmarks, task performance, capability trajectories |
| **[Economic & Labor](/knowledge-base/metrics/economic-labor/)** | Investment, automation, productivity impacts |
| **[Safety Research](/knowledge-base/metrics/safety-research/)** | Researcher headcount, funding, publication rates |
| **[Alignment Progress](/knowledge-base/metrics/alignment-progress/)** | Interpretability, robustness, alignment tax |
| **[Governance & Policy](/knowledge-base/metrics/governance-policy/)** | Regulations, enforcement, international agreements |
| **[Lab Behavior](/knowledge-base/metrics/lab-behavior/)** | RSP compliance, safety practices, transparency |
| **[Public Opinion](/knowledge-base/metrics/public-opinion/)** | Awareness, concern, trust levels |
| **[Expert Opinion](/knowledge-base/metrics/expert-opinion/)** | P(doom), timelines, researcher surveys |
| **[Geopolitics](/knowledge-base/metrics/geopolitics/)** | US-China dynamics, talent flows, coordination |
| **[Structural Indicators](/knowledge-base/metrics/structural/)** | Information quality, institutional capacity, resilience |

---

## How to Use This Section

### For Researchers
- Find current best estimates for key parameters
- Identify data gaps and measurement challenges
- Track changes over time

### For Forecasters
- Input variables for models and predictions
- Base rates and reference classes
- Uncertainty ranges and confidence intervals

### For Policymakers
- Evidence base for regulatory decisions
- Monitoring indicators for AI governance
- International comparison data

---

## Data Quality Notes

Metrics vary significantly in:
- **Availability**: Some are publicly tracked; others require inference
- **Reliability**: Some come from rigorous measurement; others from surveys or estimates
- **Timeliness**: Some update continuously; others are snapshots
- **Comparability**: Definitions and methodologies may differ across sources

Each page notes data quality and limitations for specific metrics.

---

## Key Sources

| Source | Coverage |
|--------|----------|
| **[Epoch AI](https://epochai.org/)** | Compute trends, notable models database |
| **[AI Index (Stanford HAI)](https://aiindex.stanford.edu/)** | Comprehensive annual report |
| **[State of AI Report](https://www.stateof.ai/)** | Industry trends, research progress |
| **[CAIS Surveys](https://www.safe.ai/)** | Expert opinion on AI risk |
| **[Our World in Data](https://ourworldindata.org/artificial-intelligence)** | Long-term trends, public data |
| **[Metaculus](https://www.metaculus.com/)** | Forecasts on AI milestones |
| **[AI Safety Papers Database](https://db.aisafetycommunity.org/)** | Safety research tracking |
